# langgraph_usecases/human_in_loop_content.py
from typing import TypedDict, Annotated, List, Optional, Literal
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.types import interrupt, Command # Import interrupt and Command
# from langchain_openai import ChatOpenAI # Example model

# --- State Definition ---
class ContentGenerationState(TypedDict):
    """Represents the state of the content generation process."""
    topic: str # Input topic for the content
    draft_content: Optional[str] # The initial draft generated by the AI
    human_review_feedback: Optional[str] # Feedback or approval from the human
    final_content: Optional[str] # The finalized content after review
    status: Literal["drafting", "pending_review", "finalizing", "done", "rejected"]

# --- Placeholder Models ---
# generation_model = ChatOpenAI(model="gpt-4o-mini")

# --- Node Functions ---

def draft_content(state: ContentGenerationState) -> dict:
    """Generates the initial draft content based on the topic."""
    print("---CONTENT GENERATOR: Drafting Content---")
    topic = state['topic']
    print(f"Drafting content for topic: {topic}")
    # Placeholder: Simulate content generation (LLM call)
    # draft = generation_model.invoke(f"Write a short paragraph about {topic}.").content
    draft = f"This is a draft paragraph about {topic}. It contains initial thoughts and ideas that require review."
    print("Draft generated.")
    return {"draft_content": draft, "status": "pending_review"}

def human_review(state: ContentGenerationState) -> dict:
    """
    Pauses execution to allow a human to review the draft content.
    """
    print("---CONTENT GENERATOR: Pending Human Review---")
    draft = state.get('draft_content')
    print("Presenting draft for human review...")

    # Interrupt execution and wait for human input via Command(resume=...)
    # The dictionary passed to interrupt is surfaced to the client.
    review_result = interrupt({
        "draft_content": draft,
        "instructions": "Please review the draft. Provide 'approve' or feedback for revision."
    })

    # When resumed, review_result contains the value from Command(resume=...)
    print(f"Received human review: {review_result}")

    # Determine next status based on review
    if isinstance(review_result, str) and review_result.lower() == "approve":
        print("Draft approved by human.")
        return {"human_review_feedback": "approved", "status": "finalizing"}
    elif isinstance(review_result, str): # Assume any other string is feedback for revision
        print("Received revision feedback from human.")
        # Here, you could loop back to drafting or handle rejection
        # For simplicity, we'll just store feedback and potentially reject/end
        return {"human_review_feedback": review_result, "status": "rejected"} # Or loop back
    else:
        print("Invalid review received.")
        return {"human_review_feedback": "invalid_review", "status": "rejected"}


def finalize_content(state: ContentGenerationState) -> dict:
    """
    Finalizes the content. If approved, uses the draft. If rejected, notes rejection.
    (In a more complex graph, this could incorporate feedback for revision).
    """
    print("---CONTENT GENERATOR: Finalizing Content---")
    status = state.get("status")
    draft = state.get("draft_content")
    feedback = state.get("human_review_feedback")

    if status == "finalizing" and feedback == "approved":
        print("Finalizing approved content.")
        final_content = draft # Use the approved draft
        final_status = "done"
    else: # Rejected or invalid state
        print("Content rejected or process aborted.")
        final_content = f"Content generation process stopped. Last draft:\n{draft}\nFeedback: {feedback}"
        final_status = "rejected"

    return {"final_content": final_content, "status": final_status}

# --- Routing Function ---
def route_after_review(state: ContentGenerationState) -> Literal["finalize_content", "__end__"]:
    """Decides where to go after the human review step."""
    print("---CONTENT GENERATOR: Routing after review---")
    status = state.get("status")
    if status == "finalizing" or status == "rejected":
        print("Routing to finalize content.")
        return "finalize_content"
    else:
        # Should not happen in this simple flow, but could loop back to draft
        print("Unexpected status after review, ending.")
        return END

# --- Graph Definition ---
content_workflow = StateGraph(ContentGenerationState)

content_workflow.add_node("draft_content", draft_content)
content_workflow.add_node("human_review", human_review)
content_workflow.add_node("finalize_content", finalize_content)

content_workflow.add_edge(START, "draft_content")
content_workflow.add_edge("draft_content", "human_review")
content_workflow.add_conditional_edges(
    "human_review",
    route_after_review,
    {
        "finalize_content": "finalize_content",
        END: END
    }
)
content_workflow.add_edge("finalize_content", END)

# --- Compile the Graph ---
# Requires a checkpointer for interrupt to work
# from langgraph.checkpoint.memory import MemorySaver
# checkpointer = MemorySaver()
# human_in_loop_content_agent = content_workflow.compile(checkpointer=checkpointer)
human_in_loop_content_agent = content_workflow.compile() # Needs checkpointer passed at runtime


# Example Invocation (Conceptual)
# if __name__ == "__main__":
#     from langgraph.checkpoint.memory import MemorySaver
#     import uuid
#     memory = MemorySaver()
#     thread_id = str(uuid.uuid4())
#     config = {"configurable": {"thread_id": thread_id}}
#     initial_state = {
#         "topic": "LangGraph Human-in-the-Loop",
#     }
#     print("Starting graph run...")
#     # Run until the interrupt
#     for event in human_in_loop_content_agent.stream(initial_state, config=config):
#         print(event)
#         print("---")

#     # Check the state if needed
#     # current_state = human_in_loop_content_agent.get_state(config)
#     # print("Current State:", current_state.values)

#     print("\nGraph paused for review. Resuming with 'approve'...")
#     # Resume with approval
#     for event in human_in_loop_content_agent.stream(Command(resume="approve"), config=config):
#         print(event)
#         print("---")

#     final_state = human_in_loop_content_agent.get_state(config)
#     print("\nFinal Content:", final_state.values.get('final_content'))